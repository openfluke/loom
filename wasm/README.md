# LOOM WASM Module

WebAssembly bindings for the LOOM neural network framework, enabling transformer inference and neural network training directly in the browser with zero dependencies.

## ðŸŽ‰ Transformer Inference Working!

**SmolLM2-135M-Instruct successfully running in browser WASM!**

Example generation from `Once upon a time`:

```
hi

I'm excited to see what you come up with! Let me know if you have any
```

âœ… **Full sequence context processing** (no KV cache yet, but works correctly!)  
âœ… **Pure Go implementation** (zero Python/C dependencies)  
âœ… **Loads from local files** (models downloaded via `huggingface-cli`)  
âœ… **Interactive web UI** with model selection and generation controls

### Quick Start: Transformer Inference

1. **Download a model:**

```bash
huggingface-cli download HuggingFaceTB/SmolLM2-135M-Instruct \
  --local-dir models/SmolLM2-135M-Instruct \
  --include "*.json" "*.safetensors"
```

2. **Create symlink for web server:**

```bash
cd wasm
ln -sf ../models models
```

3. **Run the server:**

```bash
bash serve_wasm.sh
# Open http://localhost:8888/inference.html
```

### Supported Models (32-bit WASM, 4GB limit)

| Model                        | Parameters | Memory | Status         |
| ---------------------------- | ---------- | ------ | -------------- |
| **SmolLM2-135M-Instruct** â­ | 135M       | ~1.5GB | âœ… **WORKING** |
| Pythia-70M                   | 70M        | ~0.5GB | âœ… Supported   |
| Pythia-160M                  | 160M       | ~1.2GB | âœ… Supported   |
| SmolLM2-360M-Instruct        | 360M       | ~4.3GB | âŒ OOM         |

**Note:** Generation is slow (reprocesses full sequence each token). KV caching will be added for speed.

## Neural Network Features

- âœ… **6.4MB Binary**: Complete framework + transformer inference in a single WASM module
- âœ… **7 Layer Types (All CPU)**: Dense, Conv2D, Multi-Head Attention, LayerNorm, RNN, LSTM, Softmax (10 variants)
- âœ… **Full CPU Implementation**: Every layer works with complete forward/backward passes
- âœ… **Registry-based Initialization**: Dynamic layer creation via `CallLayerInit()` with zero manual exports
- âœ… **24+ Methods**: All Network methods automatically exposed via reflection
- âœ… **CPU-based Neural Networks**: Create and train networks in the browser
- âœ… **Full Training Support**: `network.Train()` API with automatic gradient computation
- âœ… **Runtime Introspection**: Query methods, signatures, and parameters at runtime
- âœ… **Model Serialization**: Save/load models as JSON strings (no file system needed)
- âœ… **Type Conversion**: Automatic JavaScript â†” Go type conversion (structs, slices, custom types)
- âœ… **Zero Dependencies**: Pure WASM + Go stdlib, no external libraries
- âš ï¸ **GPU Support**: Coming soon via WebGPU (CPU-only for now)

## Quick Start

### Building

```bash
cd wasm
./build.sh
```

This produces:

- `loom.wasm` (5.4MB) - The compiled WebAssembly binary
- `wasm_exec.js` (17KB) - Go's WASM runtime

### Running the Demos

```bash
./serve.sh  # Starts server on port 8080
# Open http://localhost:8080/example.html
# Open http://localhost:8080/all_layers_test.html
```

**Demos included:**

- `example.html` - Network creation, training, introspection
- `all_layers_test.html` - âœ¨ **Load complete models from JSON!**

### âœ¨ All Layers Test Demo

The `all_layers_test.html` demo showcases the **one-line model loading** feature:

````javascript
// Load a complete model (structure + all weights) in ONE LINE!
## Transformer Inference API

### JavaScript API

```javascript
// 1. Load tokenizer
const tokenizerData = new Uint8Array(await (await fetch('models/SmolLM2-135M-Instruct/tokenizer.json')).arrayBuffer());
const tokResult = JSON.parse(LoadTokenizerFromBytes(tokenizerData));

// 2. Load transformer model
const configData = new Uint8Array(await (await fetch('models/SmolLM2-135M-Instruct/config.json')).arrayBuffer());
const weightsData = new Uint8Array(await (await fetch('models/SmolLM2-135M-Instruct/model.safetensors')).arrayBuffer());
const modelResult = JSON.parse(LoadTransformerFromBytes(configData, weightsData));

// 3. Generate text
const result = JSON.parse(GenerateText("Once upon a time", 50, 0.7));
console.log(result.generated_text);
````

### Available Functions

- `LoadTokenizerFromBytes(tokenizerData)` - Load BPE tokenizer from tokenizer.json
- `LoadTransformerFromBytes(configData, weightsData)` - Load transformer from config + safetensors
- `EncodeText(text, addSpecialTokens)` - Tokenize text to token IDs
- `DecodeTokens(tokenIDs, skipSpecialTokens)` - Convert token IDs back to text
- `GenerateText(prompt, maxTokens, temperature)` - Generate text (auto-handles tokenization)
- `GenerateNextToken(tokenIDs, temperature)` - Generate single next token

## Neural Network API

### Building

```bash
cd wasm
./build.sh
```

This produces:

- `loom.wasm` (6.0MB) - The compiled WebAssembly binary with transformer support
- `wasm_exec.js` (17KB) - Go's WASM runtime

### Running Traditional NN Demos

```bash
./serve.sh  # Starts server on port 8080
# Open http://localhost:8080/example.html
# Open http://localhost:8080/all_layers_test.html
```

**Demos included:**

- `example.html` - Network creation, training, introspection
- `all_layers_test.html` - âœ¨ **Load complete models from JSON!**
- `inference.html` - ðŸš€ **Transformer text generation!**

### âœ¨ Model Loading (The Easy Way)

```javascript
// Load complete model from JSON string
const network = LoadModelFromString(modelJSONString, "model_id");

// That's it! Network has all layers + weights loaded
// Use it immediately:
const output = JSON.parse(network.ForwardCPU(JSON.stringify([inputData])))[0];

// Train it:
const batches = [{ Input: inputData, Target: targetData }];
const config = {
  Epochs: 10,
  LearningRate: 0.01,
  LossType: "mse",
};
network.Train(JSON.stringify([batches, config]));

// Save it:
const savedJSON = JSON.parse(
  network.SaveModelToString(JSON.stringify(["model_id"]))
)[0];
```

### Creating Networks from Scratch

```javascript
// Create a network: 784 input â†’ 392 hidden â†’ 10 output
const network = NewNetwork(784, 1, 1, 2);

// Use registry-based initialization for all layer types
const layer0Config = CallLayerInit(
  "InitDenseLayer",
  JSON.stringify([784, 392, 0]) // ReLU activation
);
const layer1Config = CallLayerInit(
  "InitDenseLayer",
  JSON.stringify([392, 10, 1]) // Sigmoid activation
);

// Apply configurations to network
network.SetLayer(JSON.stringify([0, 0, 0, JSON.parse(layer0Config)]));
network.SetLayer(JSON.stringify([0, 0, 1, JSON.parse(layer1Config)]));
```

### Runtime Introspection

Discover all available methods at runtime:

```javascript
// Get all methods with metadata
const methodsJSON = network.GetMethods();
const methods = JSON.parse(methodsJSON);

console.log(`Network has ${methods.length} methods`);

methods.forEach((method) => {
  const params = method.parameters
    .map((p) => `${p.name}: ${p.type}`)
    .join(", ");
  const returns = method.returns.join(", ");
  console.log(`${method.method_name}(${params}) -> ${returns}`);
});

// Check if specific method exists
if (network.HasMethod("ForwardCPU")) {
  const sig = network.GetMethodSignature(JSON.stringify(["ForwardCPU"]));
  console.log("Signature:", sig);
  // Output: "ForwardCPU([]float32) ([]float32, time.Duration)"
}

// List all method names
const names = JSON.parse(network.ListMethods());
console.log("Available:", names);
// Output: ["BackwardCPU", "BackwardGPU", "ForwardCPU", "ForwardGPU", ...]
```

### Forward Pass

```javascript
// Create input (784 values for MNIST-sized input)
const input = new Array(784).fill(0).map(() => Math.random());

// Run forward pass
const resultJSON = network.ForwardCPU(JSON.stringify([input]));
const result = JSON.parse(resultJSON);

const output = result[0]; // Output array (10 values)
const duration = result[1]; // Execution time (nanoseconds)

console.log("Output:", output);
console.log("Time:", duration / 1e6, "ms");
console.log("Mean:", output.reduce((a, b) => a + b) / output.length);
```

### Training Loop

```javascript
// Manual training (forward passes only)
for (let epoch = 0; epoch < 5; epoch++) {
  const input = new Array(784).fill(0).map(() => Math.random());
  const resultJSON = network.ForwardCPU(JSON.stringify([input]));
  const output = JSON.parse(resultJSON)[0];

  // Compute loss (MSE against zero)
  const loss = output.reduce((sum, val) => sum + val * val, 0) / output.length;

  console.log(`Epoch ${epoch + 1}: Loss = ${loss.toFixed(6)}`);

  // For full training, you would:
  // 1. Compute gradients from loss
  // 2. Call BackwardCPU with gradients
  // 3. Update weights (manual or via Train method)
}
```

### Save/Load Models

```javascript
// Save model to JSON string
const modelJSON = network.SaveModelToString(JSON.stringify(["my_model"]));
const model = JSON.parse(JSON.parse(modelJSON)[0]);

console.log("Model size:", JSON.stringify(model).length, "bytes");
console.log("Layers:", model.models[0].cfg.layers.length);

// Store in localStorage (persistent across page reloads)
localStorage.setItem("loom_model", JSON.stringify(model));

// Load model from localStorage
const savedModel = JSON.parse(localStorage.getItem("loom_model"));
const loadedNetwork = LoadModelFromString(
  JSON.stringify(savedModel),
  "my_model"
);

// Verify loaded model works
const testInput = new Array(784).fill(0).map(() => Math.random());
const output = JSON.parse(
  loadedNetwork.ForwardCPU(JSON.stringify([testInput]))
)[0];
console.log("Loaded model output:", output);
```

### Helper Functions

```javascript
// Create layer configurations
const denseConfigJSON = InitDenseLayer(784, 128, 0); // 0 = ScaledReLU
const denseConfig = JSON.parse(denseConfigJSON);

// Create a multi-head attention layer
const mhaConfigJSON = InitMultiHeadAttentionLayer(
  512, // dModel
  8, // numHeads
  32, // batchSize
  256 // seqLength
);
const mhaConfig = JSON.parse(mhaConfigJSON);
```

## Training Example

```javascript
// Create network
const network = NewNetwork(784, 1, 1, 2);

// Training parameters
const epochs = 10;
const learningRate = 0.01;
const batchSize = 32;

// Training data (simplified)
const trainData = generateTrainingData(); // Your data

for (let epoch = 0; epoch < epochs; epoch++) {
  let totalLoss = 0;

  for (let i = 0; i < trainData.length; i += batchSize) {
    const batch = trainData.slice(i, i + batchSize);

    // Forward pass
    batch.forEach((sample) => {
      const input = sample.input;
      const target = sample.target;

      // Forward
      const outputJSON = network.ForwardCPU(JSON.stringify([input]));
      const output = JSON.parse(outputJSON)[0];

      // Compute loss (MSE example)
      const loss =
        output.reduce(
          (sum, val, idx) => sum + Math.pow(val - target[idx], 2),
          0
        ) / output.length;
      totalLoss += loss;

      // Compute gradient
      const gradOutput = output.map(
        (val, idx) => (2 * (val - target[idx])) / output.length
      );

      // Backward
      network.BackwardCPU(JSON.stringify([gradOutput]));
    });
  }

  console.log(`Epoch ${epoch + 1}: Loss = ${totalLoss / trainData.length}`);
}
```

## Method Calling Convention

All methods follow this pattern:

```javascript
// Parameters are passed as JSON array
const params = [param1, param2, param3];
const paramsJSON = JSON.stringify(params);

// Call the method
const resultJSON = network.MethodName(paramsJSON);

// Parse results (returns are also JSON array)
const results = JSON.parse(resultJSON);
const result1 = results[0];
const result2 = results[1];
```

## Type Conversion

JavaScript types are automatically converted to Go types:

| JavaScript Type | Go Type                                              |
| --------------- | ---------------------------------------------------- |
| `number`        | `int`, `float32`, `float64`                          |
| `boolean`       | `bool`                                               |
| `string`        | `string`                                             |
| `Array`         | `[]T` (slice)                                        |
| `Object`        | `map[string]T` or `struct`                           |
| Custom integers | `LayerType`, `ActivationType` (automatic conversion) |

For complex types (structs), pass as objects:

```javascript
const config = {
  Type: 0, // LayerType (automatically converted to Go type)
  Activation: 1, // ActivationType
  Kernel: [1.0, 2.0], // []float32
  Bias: [0.1, 0.2], // []float32
};

network.SetLayer(JSON.stringify([0, 0, 0, config]));
```

The WASM wrapper handles:

- âœ… **Nil values**: JavaScript `null` â†’ Go `nil` for optional fields
- âœ… **Custom types**: Type conversion for enums like `LayerType`, `ActivationType`
- âœ… **Nested structs**: Recursive conversion of complex objects
- âœ… **Slices**: Multi-dimensional arrays properly converted

## Demo Results

The included `example.html` demo successfully demonstrates:

### Network Creation

```
Network: 784 â†’ 392 â†’ 10
Layer 0: 307,328 weights initialized
Layer 1: 3,920 weights initialized
Total: 2 layers with real weights
```

### Forward Pass

```
Input: 784 random values [0, 1]
Output: [0.3456, 0.6710, 0.4669, 0.5165, 0.5758, 0.6556, 0.5595, 0.6136, 0.6537, 0.3036]
Range: [0.3036, 0.6710]
Mean: 0.5362
```

### Training (5 epochs)

```
Epoch 1/5: Loss = 0.2946
Epoch 2/5: Loss = 0.2401 âœ“ (improving)
Epoch 3/5: Loss = 0.2857
Epoch 4/5: Loss = 0.3392
Epoch 5/5: Loss = 0.3121
```

### Model Serialization

```
Saved: 1,486 bytes
Loaded: Successfully restored network
Verified: Forward pass produces identical outputs
```

## Current Limitations

- **No KV Cache**: Transformer generation reprocesses full sequence each token (slow but correct)
- **No GPU Support**: WebGPU integration coming soon
- **CPU Only**: All operations run on the CPU
- **Performance**: WASM is 2-3x slower than native Go (but instant deployment!)
- **Memory Limit**: 4GB for standard WASM (32-bit addressing)
- **Binary Size**: 6.0MB (includes full framework + transformer support)

## Future Enhancements

- [ ] **KV Caching** for transformers (10-100x speedup)
- [ ] WebGPU integration for GPU acceleration
- [ ] Memory64 support (unlimited memory when Go supports it)
- [ ] Streaming model loading for large models
- [ ] Web Workers for parallel training
- [ ] Quantization (int8/int4) for smaller models
- [ ] Optimize binary size (tree shaking, compression)
- [ ] Model visualization tools
- [ ] Performance benchmarking tools

## Architecture

### Transformer Inference

The transformer inference system (`wasm/inference.go`) provides:

1. **Byte-based Loading**: Load models from `Uint8Array` (no file system needed)
2. **Pure Go BPE Tokenizer**: Complete tokenizer implementation in `/tokenizer` package
3. **Safetensors Support**: Direct loading of HuggingFace model weights
4. **Full Sequence Context**: Processes entire token sequence for proper attention
5. **Auto-architecture Detection**: Supports LLaMA, GPT-2, and other architectures

### Neural Network Architecture

The WASM module uses reflection to automatically wrap all public methods of the `nn.Network` struct:

1. **Introspection** (`nn/introspection.go`): Discovers methods and signatures via reflection
2. **Method Wrapper** (`main.go`): Dynamically wraps methods for JavaScript calls
3. **Type Conversion** (`convertParameter`): Converts between JavaScript and Go types
4. **Result Serialization** (`serializeResults`): Returns results as JSON arrays

This approach means:

- âœ… **Zero manual bindings** - new methods automatically available
- âœ… **24+ methods exposed** - all Network methods callable from JavaScript
- âœ… **Type-safe** - runtime validation with helpful error messages
- âœ… **Self-documenting** - introspection reveals complete API

## Examples

### Transformer Inference Demo

The `inference.html` demo showcases transformer text generation in the browser:

**Features:**

- ðŸš€ Load models from local files (downloaded via `huggingface-cli`)
- ðŸŽ¨ Beautiful gradient UI with model selection cards
- âš¡ Real-time text generation with progress tracking
- ðŸ“Š Live statistics (tokens/sec, time elapsed)
- ðŸ”§ Adjustable temperature and max tokens

**Example Output (SmolLM2-135M-Instruct):**

```
Prompt: "Once upon a time"
Generated: "hi

I'm excited to see what you come up with! Let me know if you have any"
```

### Traditional Neural Network Demos

See `example.html` for a complete interactive demo including:

- âœ… Network creation with layer initialization
- âœ… Method introspection (24 methods discovered)
- âœ… Forward/backward passes with real outputs
- âœ… Model save/load with localStorage
- âœ… Training workflow with loss tracking
- âœ… Console logging for debugging

**Try it now:**

```bash
# Transformer inference:
cd wasm
bash serve_wasm.sh
# Open http://localhost:8888/inference.html

# Traditional neural networks:
cd wasm
./serve.sh
# Open http://localhost:8080/example.html
# Open http://localhost:8080/all_layers_test.html
```

## Browser Compatibility

Requires WebAssembly support (all modern browsers):

- Chrome/Edge 57+
- Firefox 52+
- Safari 11+

Tested and working on:

- âœ… Firefox 120+ (Linux)
- âœ… Chrome 119+ (Linux, macOS, Windows)

## License

Apache License 2.0 - see [LICENSE](../LICENSE) file for details.
