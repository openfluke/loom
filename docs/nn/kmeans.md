# KMeans Layer (Differentiable Clustering)

The **KMeansLayer** is a differentiable clustering layer that learns to organize inputs into meaningful clusters through backpropagation. Unlike traditional K-Means which uses discrete assignment, this layer uses **soft assignments** via softmax, making it fully differentiable and trainable end-to-end.

---

## The Core Idea

Traditional K-Means clustering assigns each input to exactly one cluster (hard assignment). But hard assignments aren't differentiableâ€”you can't backpropagate through "pick the closest one."

Loom's KMeansLayer solves this with **soft assignments**: instead of picking one cluster, we compute a probability distribution over all clusters. Closer clusters get higher probabilities.

```
Traditional K-Means (Hard):                Loom KMeans (Soft):

    Input: [0.5, 0.3]                         Input: [0.5, 0.3]
           â”‚                                         â”‚
           â–¼                                         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Distance to â”‚                          â”‚  Distance to â”‚
    â”‚  each center â”‚                          â”‚  each center â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                                         â”‚
           â–¼                                         â–¼
    Cluster 0: 0.2                            Cluster 0: 0.2
    Cluster 1: 0.8  â† closest                 Cluster 1: 0.8
    Cluster 2: 0.5                            Cluster 2: 0.5
           â”‚                                         â”‚
           â–¼                                         â–¼
    Output: [0, 1, 0]                         Output: [0.45, 0.10, 0.45]
    (one-hot, not differentiable)             (soft probabilities, differentiable!)
```

---

## Architecture: How It Actually Works

The KMeansLayer has two main components:

1. **Sub-Network**: Any neural network layer (Dense, Conv, RNN, etc.) that transforms raw input into features
2. **Cluster Centers**: Learnable vectors that represent "prototype" points in feature space

```
                              KMeansLayer
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                                                     â”‚
    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
    â”‚   â”‚                     Sub-Network                             â”‚   â”‚
    â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚   â”‚
    â”‚   â”‚  â”‚ Dense   â”‚â”€â”€â–¶â”‚ ReLU    â”‚â”€â”€â–¶â”‚ Dense   â”‚â”€â”€â–¶ Features       â”‚   â”‚
    â”‚   â”‚  â”‚ 64â†’32   â”‚   â”‚         â”‚   â”‚ 32â†’16   â”‚    [16 dims]      â”‚   â”‚
    â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚   â”‚
    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
    â”‚                                                    â”‚                â”‚
    â”‚                                                    â–¼                â”‚
    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
    â”‚   â”‚                   Distance Computation                     â”‚    â”‚
    â”‚   â”‚                                                            â”‚    â”‚
    â”‚   â”‚  Features â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚    â”‚
    â”‚   â”‚      [16]       â”‚         â”‚         â”‚         â”‚            â”‚    â”‚
    â”‚   â”‚                 â–¼         â–¼         â–¼         â–¼            â”‚    â”‚
    â”‚   â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚    â”‚
    â”‚   â”‚           â”‚Center 0â”‚ â”‚Center 1â”‚ â”‚Center 2â”‚ â”‚Center 3â”‚      â”‚    â”‚
    â”‚   â”‚           â”‚  [16]  â”‚ â”‚  [16]  â”‚ â”‚  [16]  â”‚ â”‚  [16]  â”‚      â”‚    â”‚
    â”‚   â”‚           â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      â”‚    â”‚
    â”‚   â”‚               â”‚          â”‚          â”‚          â”‚           â”‚    â”‚
    â”‚   â”‚          dist=0.2   dist=0.8   dist=0.3   dist=0.5        â”‚    â”‚
    â”‚   â”‚               â”‚          â”‚          â”‚          â”‚           â”‚    â”‚
    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
    â”‚                   â”‚          â”‚          â”‚          â”‚                â”‚
    â”‚                   â–¼          â–¼          â–¼          â–¼                â”‚
    â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
    â”‚              â”‚           Softmax(-dÂ² / 2Ï„Â²)              â”‚          â”‚
    â”‚              â”‚                                           â”‚          â”‚
    â”‚              â”‚   Small distance â†’ High probability       â”‚          â”‚
    â”‚              â”‚   Large distance â†’ Low probability        â”‚          â”‚
    â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
    â”‚                                    â”‚                                â”‚
    â”‚                                    â–¼                                â”‚
    â”‚                         Output: [0.45, 0.05, 0.35, 0.15]            â”‚
    â”‚                         (cluster assignment probabilities)          â”‚
    â”‚                                                                     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## The Math Behind Soft Assignment

For each cluster center $c_k$, we compute:

### Step 1: Squared Euclidean Distance
```
dÂ²_k = Î£(feature_i - center_k_i)Â²
```

### Step 2: Convert to Similarity (Gaussian Kernel)
```
logit_k = -dÂ²_k / (2 Ã— Ï„Â²)

Where Ï„ (tau) is the temperature parameter.
Negative distance because closer = higher similarity.
```

### Step 3: Softmax to Get Probabilities
```
P(cluster k) = exp(logit_k) / Î£ exp(logit_j)
```

Visual example:
```
Features:     [0.5, 0.3, 0.8]
Center 0:     [0.4, 0.2, 0.9]    dÂ² = 0.03    logit = -0.015    p = 0.38
Center 1:     [0.9, 0.9, 0.1]    dÂ² = 1.14    logit = -0.570    p = 0.22
Center 2:     [0.3, 0.4, 0.7]    dÂ² = 0.06    logit = -0.030    p = 0.38
                                                                 â”€â”€â”€â”€â”€
                                               Sum = 1.0          âœ“
```

---

## Temperature: Controlling Assignment Sharpness

The temperature parameter Ï„ controls how "confident" the assignments are:

```
Ï„ = 0.1 (Cold):                    Ï„ = 1.0 (Standard):                Ï„ = 3.0 (Hot):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.95  â”‚             â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   0.50  â”‚             â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     0.38  â”‚
â”‚ â–ˆ            0.03  â”‚             â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     0.30  â”‚             â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       0.32  â”‚
â”‚ â–ˆ            0.02  â”‚             â”‚ â–ˆâ–ˆâ–ˆâ–ˆ         0.20  â”‚             â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       0.30  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Almost one-hot (hard)              Soft but peaked                    Nearly uniform (soft)

Low Ï„:                             Medium Ï„:                          High Ï„:
â”œâ”€â”€ Sharp decisions                â”œâ”€â”€ Balanced                       â”œâ”€â”€ Smoother gradients
â”œâ”€â”€ Less exploration               â”œâ”€â”€ Good default                   â”œâ”€â”€ More exploration
â””â”€â”€ Can cause vanishing grads      â””â”€â”€ Start here                     â””â”€â”€ Slower to converge
```

---

## Output Modes

### Mode: `probabilities` (default)

Returns the cluster assignment probabilities directly. Good for classification or routing.

```
Input: [raw features]
           â”‚
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ KMeansLayer â”‚
    â”‚  K=4        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
Output: [0.45, 0.10, 0.35, 0.10]
         â†‘     â†‘     â†‘     â†‘
         Cluster membership probabilities
```

### Mode: `features`

Returns a weighted sum of cluster centers. Good for reconstruction or embedding.

```
Output = Î£ P(k) Ã— Center_k

       = 0.45 Ã— [0.1, 0.2, 0.9]   (Center 0)
       + 0.10 Ã— [0.8, 0.1, 0.3]   (Center 1)
       + 0.35 Ã— [0.2, 0.7, 0.5]   (Center 2)
       + 0.10 Ã— [0.6, 0.4, 0.2]   (Center 3)
       â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
       = [0.23, 0.40, 0.61]       (weighted average position)
```

---

## Backpropagation: How Learning Works

Both the **cluster centers** and the **sub-network weights** are updated through backpropagation.

```
                    Loss Gradient
                          â”‚
                          â–¼
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚   âˆ‚L/âˆ‚assignments       â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                       â”‚
              â–¼                       â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  âˆ‚L/âˆ‚centers     â”‚    â”‚  âˆ‚L/âˆ‚features    â”‚
    â”‚                  â”‚    â”‚                  â”‚
    â”‚  Update cluster  â”‚    â”‚  Backprop to     â”‚
    â”‚  positions       â”‚    â”‚  sub-network     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                       â”‚
              â–¼                       â–¼
    Centers move toward               Sub-network learns
    samples assigned to them          better features for clustering

After backprop:

    Before:                          After:
    
    â—  â—                             â—  â—
      â—     Ã— Center                     Ã— â† Center moved
    â—    â—                           â— â—
                                      â—
    
    Centers migrate toward data clusters!
```

### Gradient Through Softmax

The gradient flows back through the softmax:

```
âˆ‚L/âˆ‚logit_k = P(k) Ã— (âˆ‚L/âˆ‚P(k) - Î£ P(j) Ã— âˆ‚L/âˆ‚P(j))
```

Then through the distance computation to update centers:

```
âˆ‚L/âˆ‚center_k = (âˆ‚L/âˆ‚logit_k / Ï„Â²) Ã— (feature - center_k)
```

---

## Recursive KMeans: Building Concept Hierarchies

The real power of Loom's KMeansLayer is **recursion**. You can use a KMeansLayer as the sub-network for another KMeansLayer!

```
                        Recursive KMeans Taxonomy
                        
                              Input Image
                                   â”‚
                                   â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚        KMeans Level 1         â”‚
                    â”‚   "Is it Animal or Vehicle?"  â”‚
                    â”‚                              â”‚
                    â”‚  Center 0: Animal prototype  â”‚
                    â”‚  Center 1: Vehicle prototype â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                     â”‚
                    â–¼                     â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ KMeans Level 2 â”‚    â”‚ KMeans Level 2 â”‚
         â”‚ "Dog or Cat?"  â”‚    â”‚ "Car or Plane?"|
         â”‚                â”‚    â”‚                â”‚
         â”‚ C0: Dog proto  â”‚    â”‚ C0: Car proto  â”‚
         â”‚ C1: Cat proto  â”‚    â”‚ C1: Plane protoâ”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚                     â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
          â–¼             â–¼       â–¼             â–¼
        [Dog]        [Cat]    [Car]       [Plane]
        
Final output: Hierarchical classification with interpretable prototypes!
```

---

## Use Cases

### 1. Out-of-Distribution Detection

When an input is far from ALL cluster centers, it's likely OOD:

```
Known data:          Unknown data (OOD):
    
    â— â— â—                    â— â— â—
   â—  Ã—  â—                  â—  Ã—  â—      ?
    â— â— â—                    â— â— â—              â† Far from all centers
         â†‘                        
    Max P(k) = 0.95          Max P(k) = 0.15  â† Low confidence = OOD!
```

### 2. Interpretable Clustering

Unlike black-box features, cluster centers are actual points you can inspect:

```
Cluster Center 0:           Cluster Center 1:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Rounded shape  â”‚         â”‚  Angular shape  â”‚
â”‚  Warm colors    â”‚         â”‚  Cool colors    â”‚
â”‚  Small size     â”‚         â”‚  Large size     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“                           â†“
   "Apple-like"               "Building-like"
```

### 3. Mixture of Experts Routing

Use cluster assignments to route to different expert networks:

```
                           Input
                             â”‚
                             â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚   KMeansLayer â”‚
                     â”‚    K=3        â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
              [0.7, 0.2, 0.1] (cluster probs)
                             â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚                 â”‚                 â”‚
           â–¼                 â–¼                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Expert 0  â”‚     â”‚ Expert 1  â”‚     â”‚ Expert 2  â”‚
    â”‚ (70%)     â”‚     â”‚ (20%)     â”‚     â”‚ (10%)     â”‚
    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
          â”‚                 â”‚                 â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                   Weighted combination
```

---

## JSON Configuration

```json
{
  "type": "kmeans",
  "num_clusters": 8,
  "cluster_dim": 64,
  "distance_metric": "euclidean",
  "kmeans_temperature": 0.5,
  "kmeans_output_mode": "probabilities",
  "kmeans_learning_rate": 0.01,
  "branches": [
    {
      "type": "dense",
      "input_height": 128,
      "output_height": 64,
      "activation": "tanh"
    }
  ]
}
```

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `num_clusters` | int | required | Number of cluster centers (K) |
| `cluster_dim` | int | auto | Dimension of each center (auto-detected from sub-network output) |
| `distance_metric` | string | `"euclidean"` | Distance function: `euclidean`, `manhattan`, `cosine` |
| `kmeans_temperature` | float | `1.0` | Softmax temperature (lower = harder assignments) |
| `kmeans_output_mode` | string | `"probabilities"` | `"probabilities"` or `"features"` |
| `kmeans_learning_rate` | float | `0.01` | Learning rate for cluster center updates |
| `branches` | array | required | Sub-network configuration for feature extraction |

---

## RN Benchmark Suite: Proving the Value of KMeans

Loom includes a comprehensive benchmark suite (`tva/testing/clustering/rn*.go` and `tva/demo/kmeans/rn6.go`) that demonstrates when and why recursive KMeans outperforms standard neural networks.

### RN1: Basic Recursion Test

**Question**: Does K-Means inside K-Means actually help?

```
Task: Classify 2D points into 4 quadrants, grouped by Top/Bottom.

Data Layout:                      Architecture:
    TL (0)  â”‚  TR (1)             
    â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€              Input (2D)
            â”‚                          â”‚
    â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€                  â–¼
    BL (2)  â”‚  BR (3)             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                  â”‚ Dense 2â†’2      â”‚
Labels: TL,TR = "Top" (0)         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        BL,BR = "Bottom" (1)               â”‚
                                           â–¼
                                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                  â”‚ Inner KMeans(4) â”‚ â† Discovers 4 quadrants
                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                           â”‚
                                           â–¼
                                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                  â”‚ Outer KMeans(2) â”‚ â† Groups into Top/Bottom
                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                           â”‚
                                           â–¼
                                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                  â”‚ Dense 2â†’2      â”‚
                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Results (100 runs)**:
```
Recursive Neuro-Symbolic: 77.50% (Â±24.87%)
Standard Dense Network:   49.20% (Â±12.28%)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Winner: Loom (+28% accuracy)
```

---

### RN2: Galaxy-Star Hierarchy

**Question**: Can recursive structure learn hierarchical relationships?

```
Hierarchy:
    Galaxy 0          Galaxy 1          Galaxy 2
       â”‚                 â”‚                 â”‚
   â”Œâ”€â”€â”€â”´â”€â”€â”€â”         â”Œâ”€â”€â”€â”´â”€â”€â”€â”         â”Œâ”€â”€â”€â”´â”€â”€â”€â”
   S0   S1           S2   S3           S4   S5
   
Task: Predict Galaxy ID from point coordinates.
      Points cluster around solar systems, which cluster into galaxies.
```

**Why it's hard**: Standard networks see flat coordinates. Recursive KMeans discovers the **hierarchy automatically**.

```
Standard Dense:              Recursive KMeans:

Input: [0.3, 0.7]            Input: [0.3, 0.7]
       â”‚                            â”‚
       â–¼                            â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚Dense 12 â”‚                â”‚ KMeans(5 sys) â”‚ â† "This is Solar System 0"
  â”‚Dense 12 â”‚                â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚Dense 3  â”‚                        â”‚
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                        â–¼
       â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â–¼                     â”‚ KMeans(3 gal) â”‚ â† "System 0 is in Galaxy 0"
  "Uhh, Galaxy 1?"           â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â–¼
                             "Galaxy 0" âœ“
```

**Results**: Recursive KMeans discovers the intermediate (solar system) structure **without explicit labels**.

---

### RN3: Zero-Day Attack Detection

**Question**: Can KMeans detect inputs that don't belong to ANY learned category?

```
Training Data:                    Test Event:
                                  
   Safe        DDoS               ??? Zero-Day ???
   traffic     attack             (never seen before)
     â—           â–²                      â˜…
    â—â—â—        â–²â–²â–²                   
   â—â—â—â—â—      â–²â–²â–²â–²â–²                   

Standard Net:                     Loom KMeans:
"It's either Safe or DDoS."       "It's far from ALL my centers!"
"I'll pick DDoS (95% confident)"  "ANOMALY DETECTED (skeptical)"
        â†“                                   â†“
   HALLUCINATION                     CORRECT CAUTION
```

**How it works**:
```go
// After forward pass, check distance to ALL centers
layer := loomNet.GetLayer(0, 0, 0)
features := layer.PreActivations
centers := layer.ClusterCenters

minDist := float32(1000.0)
for k := 0; k < numCenters; k++ {
    dist := euclideanDistance(features, centers[k])
    if dist < minDist {
        minDist = dist
    }
}

if minDist > anomalyThreshold {
    // Far from ALL learned clusters = Out-of-Distribution!
    flagAsAnomaly()
}
```

**Results (100 runs)**:
```
Standard Net Hallucinations (Wrongly Confident): 0.00% (Â±0.00%)
Loom Net Anomaly Detections (Correctly Skeptical): 92.29% (Â±11.11%)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Winner: Loom detected 92% of zero-day attacks!
```

---

### RN4: Spurious Correlation Defense

**Question**: Can KMeans resist shortcut learning?

```
Training Data (with shortcut):

    Class 0                    Class 1
    â—â—â—â—â— + shortcut=0         â–²â–²â–²â–²â–² + shortcut=1
    
    Both networks learn: "If shortcut=0, predict Class 0"
    
Test Data (shortcut broken):

    Class 0                    Class 1
    â—â—â—â—â— + shortcut=RANDOM    â–²â–²â–²â–²â–² + shortcut=RANDOM
```

**Why it matters**: Real-world data often has spurious correlations (e.g., "grass" always appears with "cow" in training photos). Standard networks memorize these shortcuts. KMeans learns **geometric structure** of the actual features.

```
Standard Net:                     Loom KMeans:
                                  
  "shortcut=1? â†’ Class 1"         "This point is geometrically
   (memorized the easy path)       close to Class 0 prototype"
         â†“                                   â†“
   WRONG (50% accuracy)              CORRECT (95% accuracy)
```

**Results (100 runs)**:
```
Loom (Prototype) Net: Mean: 94.66% (Â±3.34%) | Best: 99.67%
Standard Dense Net:   Mean: 50.35% (Â±13.35%) | Best: 88.67%
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Winner: Loom resists spurious shortcuts
```

---

### RN5: Training Mode Comparison

**Question**: Which training mode works best with recursive KMeans?

Tests all 6 Loom training modes + StandardDense baseline on a hierarchical task:

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘   EXPERIMENT RN5:  The Galaxy-Star Hierarchy (All Modes)      â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Mode               â•‘ Mean Acc  â•‘  Best   â•‘  Perfect Runs  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ NormalBP           â•‘  73.16%   â•‘ 100.00% â•‘      17        â•‘
â•‘ StepBP             â•‘  74.43%   â•‘ 100.00% â•‘      20        â•‘
â•‘ Tween              â•‘  74.25%   â•‘ 100.00% â•‘      20        â•‘
â•‘ TweenChain         â•‘  74.78%   â•‘ 100.00% â•‘      20        â•‘
â•‘ StepTween          â•‘  75.23%   â•‘ 100.00% â•‘      23        â•‘
â•‘ StepTweenChain     â•‘  77.39%   â•‘ 100.00% â•‘      30   â†BESTâ•‘
â•‘ StandardDense      â•‘  29.41%   â•‘ 100.00% â•‘       8        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**Key insight**: `StepTweenChain` + KMeans = Best combination for hierarchical tasks.

---

### RN6: The Full Taxonomy Test

**Question**: Can KMeans learn a biological taxonomy with minimal data?

```
Hierarchy (3 levels):

    Kingdom: Plant (0)               Kingdom: Animal (1)
              â”‚                                â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  Flower           Tree            Bird             Mammal
    â”‚                â”‚               â”‚                  â”‚
  â”Œâ”€â”´â”€â”            â”Œâ”€â”´â”€â”           â”Œâ”€â”´â”€â”              â”Œâ”€â”´â”€â”
Rose Sunflower   Oak  Pine       Eagle Owl         Wolf Lion
```

**Architecture**:
```
Input (32D traits)
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Species KMeans (8)  â”‚ â† Discovers 8 species prototypes
â”‚   mode: "features"  â”‚    (Rose, Sunflower, Oak, ...)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Kingdom KMeans (2)  â”‚ â† Groups into Plant vs Animal
â”‚   mode: "probs"     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Dense 2â†’2           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Four challenges tested**:

| Challenge | Standard Dense | Loom Recursive |
|-----------|----------------|----------------|
| **Interpretability** | 0% (black box) | 100% (centroids = prototypes) |
| **OOD Detection** | Confident mistakes | Distance spikes detected |
| **Sample Efficiency** | Needs >100 samples | Works with 5 samples |
| **Stability** | Vanishing gradients | Stable (via Tweening) |

**The Hallucination Gap**:
```
Input: ğŸ„ Unknown Mushroom (never seen in training)

Standard Dense Output: [0.9999, 0.00002]  â† "99.99% confident it's a Plant!"
                       (WRONG - hallucinating)

Loom KMeans Output:    [0.945, 0.054]     â† Far from all centers
                       + Distance spike detected â†’ "Unknown entity!"
```

---

### Running the Benchmarks

```bash
# Run all RN1-RN5 benchmarks
cd tva/testing/clustering
./run_benchmarks.sh

# Run RN6 (comprehensive taxonomy test)
cd tva/demo/kmeans
go run rn6.go
```

**Expected output summary**:
```
RN1: Recursive wins by +28% accuracy
RN2: Recursive discovers hierarchy automatically
RN3: Loom detects 92% of zero-day attacks
RN4: Loom resists spurious correlations (95% vs 50%)
RN5: StepTweenChain is the best training mode
RN6: Loom achieves interpretability + OOD detection + sample efficiency
```

---

## Go API

```go
// Create a Dense layer for feature extraction
attachedLayer := nn.LayerConfig{
    Type:         nn.LayerDense,
    InputHeight:  64,
    OutputHeight: 32,
    Activation:   nn.ActivationTanh,
}

// Create KMeans layer with 8 clusters
kmeansLayer := nn.InitKMeansLayer(
    8,                    // numClusters
    attachedLayer,        // feature extractor
    "probabilities",      // output mode
)

// Set temperature (optional)
kmeansLayer.KMeansTemperature = 0.5

// Add to network
network.SetLayer(0, 0, 0, kmeansLayer)
```

---

## Current Limitations

> [!NOTE]
> **GPU Support**: KMeansLayer currently runs on **CPU only**. GPU acceleration is planned for a future release.

> [!WARNING]  
> **Cluster Initialization**: Centers are lazily initialized on first forward pass based on input features. For best results, ensure your first batch is representative of the data distribution.
