# Neural Tween Performance Analysis

## Overview

This document analyzes the performance of Neural Tweening across all tests, particularly focusing on the WDM (Wavelength-Division Multiplexing) multi-signal training approach.

## Key Findings

### 1. The "39.9% Problem" - Models Stuck at Local Minima

In Test 12, most layer types show both Single and WDM scoring ~39.9%. This indicates:
- Models are predicting a single class (majority class baseline)
- The 4-class dataset (sum-based quadrants) has ~40% of samples in one class
- **Neither training method escapes the local minimum in 30 epochs**

This is not a WDM failure - it's insufficient training time for the data complexity.

### 2. WDM Prevents Gradient Explosion in Attention

The **only clear WDM win** (+13.5%) is on MultiHeadAttention:

```
Single-Signal: Score 26.4% (hit NaN during training)
WDM:           Score 39.9% (stable training)
```

**Why?** Attention layers have multiplicative operations (QÂ·KÂ·V). Single samples can cause extreme activations. WDM's signal averaging acts as implicit regularization, preventing explosion.

### 3. Tween vs Backprop Overall Results

| Test | Tween Wins | BP Wins | Ties |
|------|------------|---------|------|
| Test 6 (1-layer) | 4 (LSTM, LayerNorm, SwiGLU, Attention) | 2 (Dense, RNN) | 2 |
| Test 7 (5-layer) | 4 (Dense, LayerNorm, RMSNorm, LayerNorm) | 1 (Attention-NaN) | 3 |
| Test 12 WDM | 1 (Attention) | 0 | 5 |

**Pattern: Tween excels at normalization layers and recurrent structures, but struggles with basic Dense layers.**

### 4. Why WDM Doesn't Show Dramatic Improvement

1. **Signal Dilution**: Averaging forward activations from different classes creates a "blurred centroid" that doesn't represent any single class well
   
2. **Target Conflict**: Different samples want different outputs - averaging backward targets creates conflicting weight update signals

3. **Link Budget Already High**: WDM shows better depth barriers (0.85+ vs 0.35) but this doesn't translate to better classification because the issue is *what* travels, not *how much*

### 5. When WDM Helps

WDM is beneficial when:
- **Layers are unstable** (Attention, deep networks hitting NaN)
- **Noise reduction needed** (averaging smooths stochastic weight updates)
- **Speed matters** (WDM batching is 20-40% faster due to fewer weight updates per sample)

WDM doesn't help when:
- Models are stuck at local minima (more epochs needed, not more signals)
- Simple layer types (Dense) that don't benefit from averaging

## Depth Charge Analysis (Test 10)

The 20-layer depth test reveals a key issue:

```
Tween: 85.7% (with pruning, 5 layers remaining)
Backprop: 93.4% (all 20 layers, but 4.6x slower)
```

**Insight**: Tween's pruning successfully identifies and removes dead layers, but the aggressive surgery reduces capacity. When Backprop works (doesn't vanish), it outperforms pruned Tween.

## Recommendations

### For WDM to be Effective:

1. **Use for Attention-heavy architectures** - Primary benefit is stability
2. **Increase epochs** - 30 epochs insufficient for complex data
3. **Try class-stratified batching** - Group samples by class in WDM batches to create coherent signals
4. **Combine with higher learning rates** - Stability allows more aggressive updates

### For Neural Tween Generally:

1. **Best for**: LSTM, LayerNorm, normalization layers, moderate depth
2. **Caution with**: Deep Dense stacks (link budget collapses at layer 0)
3. **Enable pruning** for very deep networks to remove dead layers
4. **Consider hybrid approach**: Tween for initial epochs, then switch to Backprop for fine-tuning

## Future Work

1. **Class-aware WDM**: Batch samples by class to prevent target dilution
2. **Adaptive channel count**: Start with more channels, reduce as training progresses
3. **Residual attention**: Add skip connections to prevent attention NaN
4. **Frontier expansion**: Combine WDM with adaptive frontier to focus on healthy layers

## RMT (Resonant Multi-Pass Training) Analysis

After WDM failed, we implemented RMT - a different multi-signal approach where the **same sample** is processed multiple times with perturbations (noise, scaling, masking), all targeting the **same class**.

### The Theory (Why We Expected It To Work)

| Pass | Input Modification | Target |
|------|-------------------|--------|
| 1 | Original | Same class |
| 2 | +Gaussian noise (5%) | Same class |
| 3 | Ã—Random scale (0.9-1.1) | Same class |
| 4 | Random mask (10% zeros) | Same class |

**Expected benefit**: Data augmentation during training, creating regularization and robustness.

### The Results (Why It Actually Failed)

```
All layer types: 40.7% (same as single-signal)
LinkBudget at L0: Still 0.5 (the bottleneck remains)
```

**RMT didn't fail because of the approach - it failed because it couldn't fix the underlying problem**:

1. **Same L0 bottleneck**: Whether we perturb the input or not, Layer 0 still chokes the signal
2. **Averaging doesn't help dead layers**: If a layer can't pass information, showing it the same data 4 times doesn't help
3. **Output averaging creates noise**: Averaging 4 outputs when 3 are from perturbed inputs dilutes the "true" answer

### Root Cause: The Problem Isn't Input-Side

Both WDM and RMT tried to improve training by **modifying inputs**:
- WDM: Different samples â†’ conflicting targets
- RMT: Same sample with perturbations â†’ same dead layers

**The real issue is target propagation**, not input processing. The backward pass estimation isn't reaching Layer 0 effectively.

---

## HLT (Harmonic Layer Training) Results

We tested layer voting on targets - each layer generates an "opinion" weighted by its LinkBudget.

### The Results: Another Tie ðŸ˜…

| Layer Type | Single | HLT | Delta |
|------------|--------|-----|-------|
| Dense | 42.0% | 42.0% | +0.0% |
| SwiGLU | 42.0% | 42.0% | +0.0% |
| MHA | 27.8% | 27.8% | +0.0% |
| RNN | 86.4% | 88.0% | +1.6% |
| LSTM | 42.0% | 42.0% | +0.0% |
| Conv2D | 42.0% | 42.0% | +0.0% |

**Average Improvement: +0.27%** - Effectively no change.

### One Bright Spot: RNN

RNN showed actual learning (86.4% â†’ 88.0%)! Both methods succeeded here, suggesting:
- RNN layers work better with Neural Tweening in general
- The 160-epoch breakthrough (from 42% to 74%+) happened for both methods
- HLT didn't help or hurt - RNN just... works

### Why HLT Failed

1. **Voting was too weak**: 40% consensus + 60% hard target meant consensus barely influenced anything
2. **Dead layers can't generate useful votes**: If L0 has 0.5 budget, its "vote" is near-random
3. **Same L0 bottleneck**: Dense still stuck at `min 0.500 @L0` throughout

### Key Observation: Attention Still Explodes

Both Single and HLT hit NaN around epoch 21 for MultiHeadAttention:
```
Epoch 11: Score 68.9% (great!)
Epoch 21: NaN (explosion)
```

**WDM was actually better here** - it prevented the NaN by averaging across samples. Neither RMT nor HLT could stabilize Attention.

---

## Conclusion: The Real Problem

After testing **3 different multi-signal approaches**:

| Approach | Strategy | Result |
|----------|----------|--------|
| WDM | Multiple samples | Ties (but stabilized Attention) |
| RMT | Same sample + perturbations | Ties |
| HLT | Layer voting on targets | Ties |

**The fundamental issue isn't the training signal - it's the architecture.**

The Dense `8â†’32` adapter at Layer 0 consistently shows:
- LinkBudget: 0.500 (minimum, essentially dead)
- DepthBarrier: ~0.04 (only 4% of signal reaches output)

**What would actually help:**
1. Skip connections (bypass dead L0)
2. Residual blocks (keep signal flowing)
3. Better initialization for the 8â†’32 expansion
4. More training time (RNN needed 60+ epochs to break through)

The multi-signal experiments were useful for understanding the problem, but the fix needs to be architectural, not algorithmic.

---

## LBL (Layer-by-Layer Training) Results

We tried freezing early layers and training from output backward - unfreeze progressively.

### The Theory

```
Epoch 1-30:   Train only L6 (output)      [L0-L5 frozen]
Epoch 31-60:  Train L5+L6                 [L0-L4 frozen]  
Epoch 61-90:  Train L4+L5+L6              [L0-L3 frozen]
...until all unfrozen...
```

**Expected**: Output learns first with good signal, then each layer gets TRAINED targets.

### The Results: Catastrophic Failure

| Layer Type | Single | LBL | Delta |
|------------|--------|-----|-------|
| Dense | 43.2% | 44.5% | +1.3% |
| SwiGLU | 43.2% | 44.5% | +1.3% |
| RNN | **92.2%** | 44.5% | **-47.6%** |

**LBL destroyed RNN's breakthrough!**

### Why It Failed

1. **RNN's breakthrough happens around epoch 61-71** when ALL layers co-adapt
2. LBL froze L0-L5 during this critical window
3. Frozen layers produced random noise â†’ L6 learned garbage mapping
4. When layers unfroze, they couldn't unlearn L6's bad patterns
5. Loss stayed at ~1.14 vs Single's 0.37

**Key insight**: The "breakthrough" requires simultaneous multi-layer learning. Freezing breaks this.

---

## Curriculum (Boosted Early-Layer Rates) Results  

We tried giving higher learning rates to early layers: L0=3x, L1=2.5x, L2=2x, L3=1.5x.

### The Results: Also Failed

| Layer Type | Single | CURR | Delta |
|------------|--------|------|-------|
| Dense | 45.6% | 45.6% | 0.0% |
| SwiGLU | 45.6% | 45.6% | 0.0% |
| MHA | 25.6% | 45.6% | **+20.0%** |
| RNN | **88.5%** | 45.6% | **-42.9%** |
| LSTM | **86.9%** | 45.6% | **-41.3%** |
| Conv2D | 45.6% | 45.6% | 0.0% |

**Average: -10.68%** - Curriculum hurt more than helped!

### Why It Failed

1. **High rates killed layers**: Early layers went to `X` (dead) by epoch 31
2. **RNN/LSTM needed normal rates**: Their breakthroughs require careful weight evolution
3. **MHA was saved by accident**: High rates prevented NaN (stabilization effect)
4. **Loss stayed at ~1.6**: Never dropped below 1.1

**Key insight**: L0 isn't stuck because the rate is too low - it's stuck because the backward
target estimation can't provide useful signal. Increasing the rate just amplifies garbage.

---

## Complete Experiment Summary

| Approach | Strategy | Result | Killed RNN? |
|----------|----------|--------|-------------|
| WDM | Multiple samples | Ties | No |
| RMT | Same sample + perturbations | Ties | No |
| HLT | Layer voting on targets | Ties | No |
| **LBL** | Freeze + progressive unfreeze | **-15%** | **YES (-47.6%)** |
| **Curriculum** | Boosted early-layer rates | **-10.7%** | **YES (-42.9%)** |

**The pattern is clear**: Any modification that disrupts the natural weight evolution destroys the "breakthrough" phenomenon seen in RNN/LSTM.

---

## The Real Conclusion

After testing **5 different approaches**, we've learned:

1. **RNN/LSTM can break through** - but ONLY with standard single-signal training
2. **Breakthroughs are fragile** - freezing, boosting, or averaging kills them
3. **The L0 bottleneck is architectural** - no training modification can fix 8â†’32 Dense
4. **Attention benefits from stabilization** - WDM's averaging or Curriculum's high rates help

**What would actually work:**
1. **Skip connections** - Let signal bypass L0 entirely
2. **Match dimensions** - Use 8â†’8 instead of 8â†’32 expansion
3. **Shorter networks** - Fewer layers = less signal decay
4. **More epochs** - RNN broke through at epoch 141, not epoch 30

---

## New Approach: "Sharpening the Blurry Image"

**Insight (Test 16):** Tween gets to ~47% accuracy very quickly (faster than backprop to 40%), but then plateaus. It's like getting a blurry photo instantly - we just need to sharpen it slightly.

### Proposed Sharpening Techniques

#### 1. Error-Proportional TweenFactor
Scale tweenFactor by how wrong each output is - bigger errors get bigger pushes.

// Instead of flat tweenFactor = 0.3
for i := range output {
    error := target[i] - output[i]
    dynamicFactor := tweenFactor * math.Abs(error)  // More wrong = bigger push
    // Apply update with dynamicFactor
}



#### 2. Winner-Take-More
Amplify gap for the correct class (Ã—1.5), dampen wrong classes (Ã—0.7).

// Boost the signal for the target class
gap[targetClass] *= 1.5  // Push harder toward correct answer
for i := range gap {
    if i != targetClass {
        gap[i] *= 0.7  // Dampen wrong classes
    }
}

#### 3. Momentum on Gap Direction
Remember which direction previous gaps pointed, use momentum toward persistent patterns.

// Track gap momentum per layer
gapMomentum[layer] = 0.9*gapMomentum[layer] + 0.1*currentGap
// Use gapMomentum instead of raw gap



#### 4. Temperature Sharpening
Apply temperature < 1.0 to outputs before computing gap, sharpening probability distribution.

// Sharpen probabilities (like knowledge distillation but inverse)
temperature := 0.5  // < 1 = sharper
for i := range output {
    output[i] = exp(log(output[i]) / temperature)
}
// Then normalize and compute gap

#### 5. Layer-Specific Gap Scaling â­ (TESTED - Mixed Results)
Earlier layers get amplified gap signal since they're hardest to reach:
```go
layerBoost := 1.0 + float32(totalLayers-1-layerIdx) * 0.3
gap *= layerBoost
```

### Test Results: Layer Gap Boost (Test 16)

| Network | Before | After | Change |
|---------|--------|-------|--------|
| Dense NormTween | 47.2% | 53.4% | **+6.2%** âœ… |
| Conv2D NormTween | 68.8% | 78.8% | **+10%** âœ… |
| Conv2D StepTween | 51.4% | 71.2% | **+19.8%** âœ… |
| RNN NormTween | 47.8% | 34.0% | **-13.8%** âŒ |

**Verdict:** Works for Dense/Conv2D, hurts RNN. May need layer-type-specific boost factors.

---

## Sharpening Ideas - ALL FAILED

1. ~~**Error-Proportional TweenFactor**~~ âŒ FAILED - No breakthrough
2. ~~**Winner-Take-More**~~ âŒ FAILED - No breakthrough (~47% still)
3. ~~**Momentum on Gap Direction**~~ âŒ FAILED - No improvement (~47% still)
4. ~~**Temperature Sharpening**~~ âŒ FAILED - Made Conv2D worse (68% â†’ 27%)
5. ~~**Layer-Specific Gap Scaling**~~ âŒ FAILED - Mixed results, hurt RNN

---

## Conclusion

**All 5 sharpening ideas have been tested and failed to break the ~50% accuracy barrier.**

The fundamental limitation of Neural Tweening appears to be the **lack of gradient direction information**. Tweening knows "I'm wrong by X amount" but doesn't know "which specific weights caused the error" or "which direction to push them."

This is analogous to being blindfolded and told "you're 10 degrees off target" without knowing if you're pointing too far left or right. The chain rule in backpropagation provides this directional information; Tweening does not.

**Use cases where Tweening may still be valuable:**
- Fast initial exploration before switching to backprop
- Online/streaming settings where speed matters more than optimal accuracy
- Reinforcement learning where approximate updates are acceptable
